---
title: "lab_12"
author: "Derek Willis"
date: "2023-05-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

* tidytext and our usual libraries

## Load libraries and establish settings

**Task** Create a codeblock and load appropriate packages and settings for this lab.

```{r}
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
library(rvest)
library(tidytext)
```

## Questions

**Q1.** You've been assigned to report a story about the leading reasons that Maryland attorneys get sanctioned by the state for misconduct. The state [publishes lists of sanctions](https://www.courts.state.md.us/attygrievance/sanctions) that contain a short text description about the situation. Load the CSV file in the data folder containing records from fiscal year 2011 onwards. Make a list of unique words from the text column, then following the example in the pre_lab, remove common "stop words" from that list and create a list of the top 10 words containing the percentage of occurrences each word represents. What's the leading word in that answer and, broadly, what do you think the top 10 words describe?

**A1.**  The top word that was identified was 'failing.' Perhaps this means that the collective theme is financial misconduct, ethical breaches, and failure to fulfill responsibilities to clients. It shows that trust account violations, mishandling of client funds, and neglect are common reasons for attorney sanctions. 

```{r}
md_attorney_sanctions<- read_csv("data/md_attorney_sanctions.csv")

md_attorney_sanctions

```
```{r}
text_data <- md_attorney_sanctions|>
  select(text) 

words <- text_data|>
  unnest_tokens(word, text)

unique_words <- md_attorney_sanctions |> select(text) |>
  unnest_tokens(word, text)

unique_words |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

**Q2.** Let's move beyond single words to phrases. Make a list of the top 10 three-word phrases, called trigrams, based on the example from the pre_lab (you'll need to modify the example code to do this). What's the top trigram and how often does it appear? What does that phrase mean in legal terms?

**A2.** attorney trust account appears 343 times, conduct involving dishonesty	appears 155 times, and dishonesty fraud deceit appears 155 times. Attorney trust account is an account that lawyers hold money in on behalf of the clients. This could mean there were issues with the lawyer and the money in the account. 

```{r}
md_attorney_sanctions |>
  unnest_tokens(trigram, text, token = "ngrams", n = 3) |>
  separate(trigram, c("word1", "word2", "word3"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  filter(!word3 %in% stop_words$word) |>
  mutate(trigram = paste(word1, word2, word3, sep=" ")) |>
  group_by(trigram) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

**Q3.** Let's drop back down to more traditional text analysis - take the top trigram from Q2 and write code to see how many times it occurs in the text column in each fiscal year. What do you think the answer produced by your code suggests? What else could you do to try and clarify the most important reasons attorneys get sanctioned?

**A3.** The most times it occurred was 2021. The results suggest that issues related to "attorney trust account" violations are a significant reason for attorney sanctions, reflecting potential non-compliance with their responsibilities or inadequate oversight. Since it is lower in some years and higher in others, this may be possibly due to increased enforcement, regulatory changes, or high-profile cases drawing attention to trust account management. 

```{r}
md_attorney_sanctions |>
  unnest_tokens(trigram, text, token = "ngrams", n = 3) |>
  separate(trigram, c("word1", "word2", "word3"), sep = " ")|>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  filter(!word3 %in% stop_words$word) |>
  mutate(trigram = paste(word1, word2, word3, sep=" ")) |>
  group_by(fiscal_year) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |> 
  top_n(12)
```
